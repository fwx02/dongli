import requests
from bs4 import BeautifulSoup
import json
import logging
import time
import datetime
import os
import sqlite3
import re

# é…ç½®å‚æ•°
DB_DIR = os.getenv("DB_DIR", ".")
DB_FILE = os.path.join(DB_DIR, "book_history.db")
LOG_FILE = os.path.join(DB_DIR, "crawler.log")
WECHAT_WORK_WEBHOOK = os.getenv("WECHAT_WORK_WEBHOOK")
MAX_MESSAGE_LENGTH = 3500
MAX_BOOKS_PER_BATCH = 15
DB_COMMIT_BATCH_SIZE = 10
MIN_INTERVAL_BETWEEN_MESSAGES = 3
LAST_MESSAGE_TIME_FILE = os.path.join(DB_DIR, "last_message_time.txt")

def setup_logging():
    """é…ç½®æ—¥å¿—è®°å½•"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(LOG_FILE, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    logging.info(f"æ•°æ®åº“æ–‡ä»¶: {DB_FILE}")
    logging.info(f"æ—¥å¿—æ–‡ä»¶: {LOG_FILE}")

def normalize_title(title):
    """è§„èŒƒåŒ–ä¹¦ç±æ ‡é¢˜ï¼Œæé«˜æŸ¥é‡å‡†ç¡®æ€§"""
    original_title = title
    title = title.lstrip('0123456789.ã€ ')  # ç§»é™¤è¡Œå·å‰ç¼€
    title = re.sub(r'[ \t]+', ' ', title)  # åˆå¹¶è¿ç»­ç©ºæ ¼
    
    # é€‰æ‹©æ€§ç§»é™¤ç‰ˆæœ¬ä¿¡æ¯ï¼ˆä¿ç•™ä¸»è¦æ ‡é¢˜ï¼‰
    title = re.sub(r'ï¼ˆé¦–åˆ·.*?ï¼‰', '', title)  # ç§»é™¤é¦–åˆ·é™å®šç­‰ä¿¡æ¯
    title = re.sub(r'\(é¦–åˆ·.*?\)', '', title)
    
    title = title.strip()
    logging.debug(f"æ ‡é¢˜è§„èŒƒåŒ–: '{original_title}' â†’ '{title}'")
    return title

def create_database():
    """åˆ›å»ºæ•°æ®åº“è¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
    try:
        os.makedirs(DB_DIR, exist_ok=True)
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS books (
                title TEXT PRIMARY KEY,
                publish_month TEXT,
                first_seen TEXT,
                last_seen TEXT,
                is_published INTEGER DEFAULT 0
            )
        ''')
        conn.commit()
        conn.close()
        logging.info("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
        
        # è°ƒè¯•ï¼šéªŒè¯è¡¨ç»“æ„
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        cursor.execute("PRAGMA table_info(books)")
        columns = cursor.fetchall()
        logging.debug(f"è¡¨ç»“æ„: {columns}")
        conn.close()
    except Exception as e:
        logging.error(f"åˆ›å»ºæ•°æ®åº“æ—¶å‡ºé”™: {str(e)}")
        raise

def check_book_exists(title):
    """æ£€æŸ¥ä¹¦ç±æ˜¯å¦å·²å­˜åœ¨äºæ•°æ®åº“ä¸­"""
    try:
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        cursor.execute("SELECT 1 FROM books WHERE title = ?", (title,))
        result = cursor.fetchone()
        conn.close()
        
        # è°ƒè¯•ï¼šè¾“å‡ºæ£€æŸ¥ç»“æœ
        exists = "å­˜åœ¨" if result else "ä¸å­˜åœ¨"
        logging.debug(f"æ£€æŸ¥ä¹¦ç± '{title}': {exists}")
        return result is not None
    except Exception as e:
        logging.error(f"æ£€æŸ¥ä¹¦ç±æ—¶å‡ºé”™: {str(e)}")
        return False

def batch_add_books(books):
    """æ‰¹é‡æ·»åŠ ä¹¦ç±åˆ°æ•°æ®åº“"""
    if not books:
        logging.info("æ²¡æœ‰æ–°ä¹¦éœ€è¦æ·»åŠ ")
        return 0
    
    try:
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        new_count = 0
        
        for i, book in enumerate(books):
            normalized_title = normalize_title(book["title"])
            
            # è°ƒè¯•ï¼šè¾“å‡ºå¤„ç†ä¸­çš„ä¹¦ç±ä¿¡æ¯
            logging.debug(f"å¤„ç†ä¹¦ç± #{i+1}/{len(books)}: '{normalized_title}'")
            
            if not check_book_exists(normalized_title):
                try:
                    cursor.execute(
                        """INSERT INTO books (title, publish_month, first_seen, last_seen, is_published) 
                           VALUES (?, ?, ?, ?, 0)""",
                        (normalized_title, book["publish_month"], book["first_seen"], book["last_seen"])
                    )
                    new_count += 1
                    logging.info(f"âœ… æ–°ä¹¦å…¥åº“: '{normalized_title}' ({book['publish_month']})")
                except sqlite3.IntegrityError as e:
                    logging.warning(f"âš ï¸ ä¹¦ç±å·²å­˜åœ¨æˆ–è¿åå”¯ä¸€çº¦æŸ: '{normalized_title}', é”™è¯¯: {str(e)}")
                except Exception as e:
                    logging.error(f"âŒ æ·»åŠ ä¹¦ç±å¤±è´¥: '{normalized_title}', é”™è¯¯: {str(e)}")
            else:
                logging.info(f"ğŸ“š ä¹¦ç±å·²å­˜åœ¨: '{normalized_title}'")
            
            # æ¯æ‰¹æäº¤ä¸€æ¬¡
            if (i + 1) % DB_COMMIT_BATCH_SIZE == 0:
                conn.commit()
                logging.debug(f"æ‰¹é‡æäº¤ {i+1} æ¡è®°å½•")
        
        conn.commit()
        conn.close()
        logging.info(f"âœ… å…±æ·»åŠ  {new_count} æœ¬æ–°ä¹¦ï¼Œå¤„ç† {len(books)} æœ¬ä¹¦ç±")
        
        # è°ƒè¯•ï¼šéªŒè¯æ’å…¥ç»“æœ
        if new_count > 0:
            conn = sqlite3.connect(DB_FILE)
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM books")
            count = cursor.fetchone()[0]
            logging.info(f"ğŸ“Š æ•°æ®åº“æ€»è®°å½•æ•°: {count}")
            conn.close()
        
        return new_count
    except Exception as e:
        logging.error(f"âŒ æ‰¹é‡æ·»åŠ ä¹¦ç±æ—¶å‡ºé”™: {str(e)}")
        conn.rollback()
        conn.close()
        return 0

def get_book_titles(page_num):
    """è·å–æŒ‡å®šé¡µçš„ä¹¦ç±æ ‡é¢˜å’Œå‡ºç‰ˆæ—¶é—´"""
    url = f"https://www.tongli.com.tw/Search1.aspx?Page={page_num}"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    
    try:
        logging.info(f"å¼€å§‹çˆ¬å–ç¬¬{page_num}é¡µ: {url}")
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        # è°ƒè¯•ï¼šè¾“å‡ºå“åº”çŠ¶æ€å’Œéƒ¨åˆ†å†…å®¹
        logging.debug(f"å“åº”çŠ¶æ€ç : {response.status_code}")
        logging.debug(f"å“åº”å†…å®¹å‰1000å­—ç¬¦: {response.text[:1000]}")
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–å‡ºä¹¦è¡¨æœˆä»½
        month_element = soup.select_one('h5.sdBook_t span#ContentPlaceHolder1_DataMonth')
        publish_month = month_element.get_text(strip=True) if month_element else "æœªçŸ¥æœˆä»½"
        logging.info(f"å½“å‰çˆ¬å–çš„æ˜¯ {publish_month} çš„å‡ºä¹¦è¡¨")
        
        book_elements = soup.select('td[data-th="æ›¸åï¼é›†æ•¸"]')
        
        # è°ƒè¯•ï¼šè¾“å‡ºæå–åˆ°çš„ä¹¦ç±æ•°é‡å’Œæ ·æœ¬
        logging.info(f"æå–åˆ° {len(book_elements)} æœ¬ä¹¦ç±æ ‡é¢˜")
        if book_elements:
            sample_titles = [el.get_text(strip=True) for el in book_elements[:3]]
            logging.info(f"æ ·æœ¬æ ‡é¢˜: {sample_titles}")
        
        # å¤„ç†ä¹¦åï¼Œç§»é™¤å¯èƒ½çš„è¡Œå·å‰ç¼€
        processed_titles = []
        for title in [element.get_text(strip=True) for element in book_elements]:
            processed_title = title.lstrip('0123456789.ã€ ')
            processed_titles.append(processed_title)
        
        return {
            "publish_month": publish_month,
            "titles": processed_titles
        }
    except Exception as e:
        logging.error(f"çˆ¬å–ç¬¬{page_num}é¡µæ—¶å‡ºé”™: {str(e)}")
        return {
            "publish_month": "æœªçŸ¥æœˆä»½",
            "titles": []
        }

# å…¶ä»–å‡½æ•°ä¿æŒä¸å˜...

def main():
    """ä¸»å‡½æ•°"""
    try:
        setup_logging()
        logging.info("ğŸ“– çˆ¬è™«ç¨‹åºå¯åŠ¨")
        
        # åˆå§‹åŒ–æ•°æ®åº“
        create_database()
        
        # çˆ¬å–æ•°æ®
        new_books = []
        current_books = []
        publish_months = set()
        
        for page in range(1, 4):
            result = get_book_titles(page)
            titles = result["titles"]
            publish_month = result["publish_month"]
            publish_months.add(publish_month)
            
            # è®°å½•æ‰€æœ‰ä¹¦ç±
            page_books = []
            today = datetime.datetime.now().strftime("%Y-%m-%d")
            
            for title in titles:
                book_info = {
                    "title": title,
                    "publish_month": publish_month,
                    "first_seen": today,
                    "last_seen": today
                }
                page_books.append(book_info)
            
            current_books.extend(page_books)
            
            # æ£€æŸ¥å¹¶è®°å½•æ–°ä¹¦
            for book in page_books:
                normalized_title = normalize_title(book["title"])
                if not check_book_exists(normalized_title):
                    new_books.append(book)
                    logging.info(f"ğŸ” å‘ç°æ–°ä¹¦: {normalized_title} ({publish_month})")
        
        # è°ƒè¯•ï¼šè¾“å‡ºçˆ¬å–ç»“æœ
        logging.info(f"ğŸ“Š çˆ¬å–å®Œæˆ: {len(current_books)} æœ¬å½“å‰ä¹¦ç±, {len(new_books)} æœ¬æ–°ä¹¦")
        
        # æ·»åŠ æ–°ä¹¦åˆ°æ•°æ®åº“
        if new_books:
            added_count = batch_add_books(new_books)
            logging.info(f"âœ… æ•°æ®åº“æ›´æ–°: æ·»åŠ äº† {added_count} æœ¬æ–°ä¹¦")
        else:
            logging.info("ğŸ“­ æ²¡æœ‰å‘ç°æ–°ä¹¦")
        
        # å…¶ä½™ä»£ç ä¿æŒä¸å˜...
        
    except Exception as e:
        logging.error(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™: {str(e)}")
        raise

if __name__ == "__main__":
    main()
