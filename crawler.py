import requests
from bs4 import BeautifulSoup
import json
import logging
import time
import datetime
import os
import sqlite3
import re

# é…ç½®å‚æ•°
DB_DIR = os.getenv("DB_DIR", ".")
DB_FILE = os.path.join(DB_DIR, "book_history.db")
LOG_FILE = os.path.join(DB_DIR, "crawler.log")
WECHAT_WORK_WEBHOOK = os.getenv("WECHAT_WORK_WEBHOOK")
MAX_MESSAGE_LENGTH = 3500
MAX_BOOKS_PER_BATCH = 15
DB_COMMIT_BATCH_SIZE = 10
MIN_INTERVAL_BETWEEN_MESSAGES = 60
LAST_MESSAGE_TIME_FILE = os.path.join(DB_DIR, "last_message_time.txt")

def setup_logging():
    """é…ç½®æ—¥å¿—è®°å½•"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(LOG_FILE, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    logging.info(f"æ•°æ®åº“æ–‡ä»¶: {DB_FILE}")
    logging.info(f"æ—¥å¿—æ–‡ä»¶: {LOG_FILE}")

def normalize_title(title):
    """è§„èŒƒåŒ–ä¹¦ç±æ ‡é¢˜ï¼Œæé«˜æŸ¥é‡å‡†ç¡®æ€§"""
    original_title = title
    title = title.lstrip('0123456789.ã€ ')  # ç§»é™¤è¡Œå·å‰ç¼€
    title = re.sub(r'[ \t]+', ' ', title)  # åˆå¹¶è¿ç»­ç©ºæ ¼
    
    # é€‰æ‹©æ€§ç§»é™¤ç‰ˆæœ¬ä¿¡æ¯ï¼ˆä¿ç•™ä¸»è¦æ ‡é¢˜ï¼‰
    title = re.sub(r'ï¼ˆé¦–åˆ·.*?ï¼‰', '', title)  # ç§»é™¤é¦–åˆ·é™å®šç­‰ä¿¡æ¯
    title = re.sub(r'\(é¦–åˆ·.*?\)', '', title)
    title = re.sub(r'ã€.*?ã€‘', '', title)  # ç§»é™¤æ–¹æ‹¬å·å†…å®¹
    
    title = title.strip()
    logging.debug(f"æ ‡é¢˜è§„èŒƒåŒ–: '{original_title}' â†’ '{title}'")
    return title

def create_database():
    """åˆ›å»ºæ•°æ®åº“è¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
    try:
        os.makedirs(DB_DIR, exist_ok=True)
        logging.info(f"æ•°æ®åº“ç›®å½•: {DB_DIR}")
        
        # éªŒè¯ç›®å½•æƒé™
        if not os.access(DB_DIR, os.W_OK):
            logging.error(f"ç›®å½•ä¸å¯å†™: {DB_DIR}")
            raise PermissionError(f"æ— æ³•å†™å…¥ç›®å½•: {DB_DIR}")
        
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS books (
                title TEXT PRIMARY KEY,
                publish_month TEXT,
                first_seen TEXT,
                last_seen TEXT,
                is_published INTEGER DEFAULT 0
            )
        ''')
        
        # éªŒè¯è¡¨æ˜¯å¦åˆ›å»ºæˆåŠŸ
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='books'")
        table_exists = cursor.fetchone()
        
        if table_exists:
            logging.info("âœ… ä¹¦ç±è¡¨åˆ›å»ºæˆåŠŸ")
        else:
            logging.error("âŒ ä¹¦ç±è¡¨åˆ›å»ºå¤±è´¥")
            raise Exception("æ•°æ®åº“è¡¨åˆ›å»ºå¤±è´¥")
        
        conn.commit()
        conn.close()
        
        # éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if os.path.exists(DB_FILE):
            logging.info(f"âœ… æ•°æ®åº“æ–‡ä»¶å·²åˆ›å»º: {DB_FILE}")
            file_permissions = oct(os.stat(DB_FILE).st_mode & 0o777)
            logging.info(f"æ–‡ä»¶æƒé™: {file_permissions}")
        else:
            logging.error(f"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {DB_FILE}")
            raise FileNotFoundError("æ•°æ®åº“æ–‡ä»¶æœªåˆ›å»º")
            
    except Exception as e:
        logging.error(f"åˆ›å»ºæ•°æ®åº“æ—¶å‡ºé”™: {str(e)}")
        raise

def check_book_exists(title):
    """æ£€æŸ¥ä¹¦ç±æ˜¯å¦å·²å­˜åœ¨äºæ•°æ®åº“ä¸­"""
    try:
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        cursor.execute("SELECT 1 FROM books WHERE title = ?", (title,))
        result = cursor.fetchone()
        conn.close()
        
        exists = "å­˜åœ¨" if result else "ä¸å­˜åœ¨"
        logging.debug(f"æ£€æŸ¥ä¹¦ç± '{title}': {exists}")
        return result is not None
    except Exception as e:
        logging.error(f"æ£€æŸ¥ä¹¦ç±æ—¶å‡ºé”™: {str(e)}")
        return False

def batch_add_books(books):
    """æ‰¹é‡æ·»åŠ ä¹¦ç±åˆ°æ•°æ®åº“"""
    if not books:
        logging.info("æ²¡æœ‰æ–°ä¹¦éœ€è¦æ·»åŠ ")
        return 0
    
    try:
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        new_count = 0
        
        for i, book in enumerate(books):
            normalized_title = normalize_title(book["title"])
            logging.debug(f"å¤„ç†ä¹¦ç± #{i+1}/{len(books)}: '{normalized_title}'")
            
            if not check_book_exists(normalized_title):
                try:
                    cursor.execute(
                        """INSERT INTO books (title, publish_month, first_seen, last_seen, is_published) 
                           VALUES (?, ?, ?, ?, 0)""",
                        (normalized_title, book["publish_month"], book["first_seen"], book["last_seen"])
                    )
                    new_count += 1
                    logging.info(f"âœ… æ–°ä¹¦å…¥åº“: '{normalized_title}' ({book['publish_month']})")
                except sqlite3.IntegrityError as e:
                    logging.warning(f"âš ï¸ ä¹¦ç±å·²å­˜åœ¨æˆ–è¿åå”¯ä¸€çº¦æŸ: '{normalized_title}', é”™è¯¯: {str(e)}")
                except Exception as e:
                    logging.error(f"âŒ æ·»åŠ ä¹¦ç±å¤±è´¥: '{normalized_title}', é”™è¯¯: {str(e)}")
            else:
                logging.info(f"ğŸ“š ä¹¦ç±å·²å­˜åœ¨: '{normalized_title}'")
            
            if (i + 1) % DB_COMMIT_BATCH_SIZE == 0:
                conn.commit()
                logging.debug(f"æ‰¹é‡æäº¤ {i+1} æ¡è®°å½•")
        
        conn.commit()
        conn.close()
        logging.info(f"âœ… å…±æ·»åŠ  {new_count} æœ¬æ–°ä¹¦ï¼Œå¤„ç† {len(books)} æœ¬ä¹¦ç±")
        
        if new_count > 0:
            conn = sqlite3.connect(DB_FILE)
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM books")
            count = cursor.fetchone()[0]
            logging.info(f"ğŸ“Š æ•°æ®åº“æ€»è®°å½•æ•°: {count}")
            conn.close()
        
        return new_count
    except Exception as e:
        logging.error(f"âŒ æ‰¹é‡æ·»åŠ ä¹¦ç±æ—¶å‡ºé”™: {str(e)}")
        conn.rollback()
        conn.close()
        return 0

def get_book_titles(page_num):
    """è·å–æŒ‡å®šé¡µçš„ä¹¦ç±æ ‡é¢˜å’Œå‡ºç‰ˆæ—¶é—´"""
    url = f"https://www.tongli.com.tw/Search1.aspx?Page={page_num}"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    
    try:
        logging.info(f"å¼€å§‹çˆ¬å–ç¬¬{page_num}é¡µ: {url}")
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        logging.debug(f"å“åº”çŠ¶æ€ç : {response.status_code}")
        logging.debug(f"å“åº”å†…å®¹å‰1000å­—ç¬¦: {response.text[:1000]}")
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        month_element = soup.select_one('h5.sdBook_t span#ContentPlaceHolder1_DataMonth')
        publish_month = month_element.get_text(strip=True) if month_element else "æœªçŸ¥æœˆä»½"
        logging.info(f"å½“å‰çˆ¬å–çš„æ˜¯ {publish_month} çš„å‡ºä¹¦è¡¨")
        
        book_elements = soup.select('td[data-th="æ›¸åï¼é›†æ•¸"]')
        
        logging.info(f"æå–åˆ° {len(book_elements)} æœ¬ä¹¦ç±æ ‡é¢˜")
        if book_elements:
            sample_titles = [el.get_text(strip=True) for el in book_elements[:3]]
            logging.info(f"æ ·æœ¬æ ‡é¢˜: {sample_titles}")
        
        processed_titles = []
        for title in [element.get_text(strip=True) for element in book_elements]:
            processed_title = title.lstrip('0123456789.ã€ ')
            processed_titles.append(processed_title)
        
        return {
            "publish_month": publish_month,
            "titles": processed_titles
        }
    except Exception as e:
        logging.error(f"çˆ¬å–ç¬¬{page_num}é¡µæ—¶å‡ºé”™: {str(e)}")
        return {
            "publish_month": "æœªçŸ¥æœˆä»½",
            "titles": []
        }

def test_database_creation():
    """æµ‹è¯•æ•°æ®åº“åˆ›å»ºåŠŸèƒ½"""
    try:
        logging.info("å¼€å§‹æµ‹è¯•æ•°æ®åº“åˆ›å»º...")
        
        # æ£€æŸ¥ç›®å½•æƒé™
        if not os.access(DB_DIR, os.W_OK):
            logging.error(f"ç›®å½•ä¸å¯å†™: {DB_DIR}")
            return False
        
        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        test_file = os.path.join(DB_DIR, "test.txt")
        with open(test_file, 'w') as f:
            f.write("æµ‹è¯•æ–‡ä»¶")
        
        if os.path.exists(test_file):
            logging.info("âœ… æµ‹è¯•æ–‡ä»¶åˆ›å»ºæˆåŠŸ")
            os.remove(test_file)
        else:
            logging.error("âŒ æ— æ³•åˆ›å»ºæµ‹è¯•æ–‡ä»¶")
            return False
        
        # åˆ›å»ºæ•°æ®åº“
        create_database()
        
        # éªŒè¯æ•°æ®åº“æ–‡ä»¶
        if os.path.exists(DB_FILE):
            logging.info("âœ… æ•°æ®åº“æ–‡ä»¶å­˜åœ¨")
            
            # ç®€å•æŸ¥è¯¢æµ‹è¯•
            conn = sqlite3.connect(DB_FILE)
            cursor = conn.cursor()
            cursor.execute("SELECT 1")
            result = cursor.fetchone()
            conn.close()
            
            if result:
                logging.info("âœ… æ•°æ®åº“æŸ¥è¯¢æµ‹è¯•æˆåŠŸ")
                return True
            else:
                logging.error("âŒ æ•°æ®åº“æŸ¥è¯¢æµ‹è¯•å¤±è´¥")
                return False
        else:
            logging.error("âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
            return False
            
    except Exception as e:
        logging.error(f"æ•°æ®åº“æµ‹è¯•å‡ºé”™: {str(e)}")
        return False

def send_combined_message(title, content):
    """å‘é€åˆå¹¶åçš„ä¼ä¸šå¾®ä¿¡æ¶ˆæ¯ï¼ˆè€ƒè™‘APIé™åˆ¶ï¼‰"""
    if not WECHAT_WORK_WEBHOOK:
        logging.warning("æœªè®¾ç½®ä¼ä¸šå¾®ä¿¡Webhookï¼Œæ— æ³•å‘é€é€šçŸ¥")
        return False
    
    # æ£€æŸ¥ä¸Šæ¬¡å‘é€æ—¶é—´ï¼Œç¡®ä¿ç¬¦åˆæœ€å°é—´éš”è¦æ±‚
    last_time = get_last_message_time()
    now = datetime.datetime.now()
    
    if last_time and (now - last_time).total_seconds() < MIN_INTERVAL_BETWEEN_MESSAGES:
        wait_time = MIN_INTERVAL_BETWEEN_MESSAGES - (now - last_time).total_seconds()
        logging.info(f"è·ç¦»ä¸Šæ¬¡å‘é€æ¶ˆæ¯æ—¶é—´ä¸è¶³ï¼Œç­‰å¾… {wait_time:.0f} ç§’")
        time.sleep(wait_time)
    
    # æ„å»ºå®Œæ•´çš„è¯·æ±‚JSONå¹¶è®¡ç®—å…¶å­—èŠ‚é•¿åº¦
    data = {
        "msgtype": "markdown",
        "markdown": {
            "content": content
        }
    }
    json_data = json.dumps(data, ensure_ascii=False).encode('utf-8')
    json_length = len(json_data)
    
    # æ£€æŸ¥JSONåºåˆ—åŒ–åçš„æ€»é•¿åº¦
    if json_length > 4096:
        logging.warning(f"å®Œæ•´JSONè¯·æ±‚é•¿åº¦ {json_length} è¶…è¿‡ä¼ä¸šå¾®ä¿¡é™åˆ¶ 4096 å­—èŠ‚")
        
        # å°è¯•æ™ºèƒ½åˆ†æ®µ
        sections = split_message_smart(content)
        success = True
        
        for i, section in enumerate(sections):
            section_title = f"{title} (åˆ†æ®µ{i+1}/{len(sections)})"
            
            # é‡æ–°è®¡ç®—åˆ†æ®µåçš„JSONé•¿åº¦
            section_data = {
                "msgtype": "markdown",
                "markdown": {
                    "content": section
                }
            }
            section_length = len(json.dumps(section_data, ensure_ascii=False).encode('utf-8'))
            
            logging.info(f"å‘é€åˆ†æ®µ {i+1}/{len(sections)}: {section_title}ï¼Œé•¿åº¦ {section_length} å­—èŠ‚")
            
            if section_length > 4096:
                logging.error(f"åˆ†æ®µ {i+1} é•¿åº¦ {section_length} ä»ç„¶è¶…è¿‡é™åˆ¶ï¼Œå°è¯•æˆªæ–­")
                section = truncate_message(section)
                section_data["markdown"]["content"] = section
                section_length = len(json.dumps(section_data, ensure_ascii=False).encode('utf-8'))
                logging.info(f"æˆªæ–­ååˆ†æ®µ {i+1} é•¿åº¦ä¸º {section_length} å­—èŠ‚")
            
            try:
                response = requests.post(
                    WECHAT_WORK_WEBHOOK, 
                    headers={'Content-Type': 'application/json'}, 
                    data=json.dumps(section_data, ensure_ascii=False), 
                    timeout=15
                )
                response.raise_for_status()
                
                result = response.json()
                if result.get("errcode") == 0:
                    logging.info(f"åˆ†æ®µ {i+1} å‘é€æˆåŠŸ")
                else:
                    logging.error(f"åˆ†æ®µ {i+1} å‘é€å¤±è´¥: {result}")
                    success = False
            except Exception as e:
                logging.error(f"å‘é€åˆ†æ®µ {i+1} æ—¶å‡ºé”™: {str(e)}")
                success = False
        
        if success:
            logging.info(f"æ¶ˆæ¯å·²æˆåŠŸåˆ†æ®µå‘é€ï¼Œå…± {len(sections)} æ®µ")
            save_last_message_time()
            return True
        else:
            logging.error("æ¶ˆæ¯åˆ†æ®µå‘é€å¤±è´¥")
            return False
    else:
        # ç›´æ¥å‘é€æ¶ˆæ¯
        logging.info(f"å‘é€æ¶ˆæ¯: {title}ï¼Œé•¿åº¦ {json_length} å­—èŠ‚")
        try:
            response = requests.post(
                WECHAT_WORK_WEBHOOK, 
                headers={'Content-Type': 'application/json'}, 
                data=json_data, 
                timeout=15
            )
            response.raise_for_status()
            
            result = response.json()
            if result.get("errcode") == 0:
                logging.info("æ¶ˆæ¯å‘é€æˆåŠŸ")
                save_last_message_time()
                return True
            else:
                logging.error(f"æ¶ˆæ¯å‘é€å¤±è´¥: {result}")
                return False
        except Exception as e:
            logging.error(f"å‘é€æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}")
            return False

def split_message_smart(content):
    """æ™ºèƒ½åˆ†å‰²é•¿æ¶ˆæ¯ï¼Œä¿æŒå†…å®¹å®Œæ•´æ€§"""
    if len(content) <= MAX_MESSAGE_LENGTH:
        return [content]
    
    sections = []
    current_section = ""
    lines = content.split('\n')
    
    for line in lines:
        # å¦‚æœæ·»åŠ å½“å‰è¡Œåè¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œåˆ™åˆ›å»ºæ–°çš„åˆ†æ®µ
        if len(current_section) + len(line) + 1 > MAX_MESSAGE_LENGTH:
            # å¦‚æœå½“å‰åˆ†æ®µä¸ºç©ºï¼Œå¼ºåˆ¶æ·»åŠ æ­¤è¡Œï¼ˆå¯èƒ½ä¼šè¶…è¿‡é™åˆ¶ï¼Œä½†è¿™æ˜¯æç«¯æƒ…å†µï¼‰
            if not current_section:
                sections.append(line)
                current_section = ""
            else:
                sections.append(current_section)
                current_section = line
        else:
            # æ·»åŠ å½“å‰è¡Œåˆ°å½“å‰åˆ†æ®µ
            if current_section:
                current_section += '\n' + line
            else:
                current_section = line
    
    # æ·»åŠ æœ€åä¸€ä¸ªåˆ†æ®µ
    if current_section:
        sections.append(current_section)
    
    return sections

def truncate_message(content):
    """æˆªæ–­æ¶ˆæ¯å†…å®¹ï¼Œç¡®ä¿ä¸è¶…è¿‡æœ€å¤§é•¿åº¦"""
    if len(content) <= MAX_MESSAGE_LENGTH:
        return content
    
    # å°è¯•åœ¨æœ€åä¸€ä¸ªå®Œæ•´çš„é¡¹ç›®ç¬¦å·æˆ–æ ‡é¢˜å¤„æˆªæ–­
    markers = ['\n- ', '\n* ', '\n# ', '\n## ', '\n### ']
    truncate_index = -1
    
    for marker in markers:
        index = content.rfind(marker, 0, MAX_MESSAGE_LENGTH)
        if index > truncate_index:
            truncate_index = index
    
    if truncate_index > 0:
        # åœ¨æ ‡è®°åæ·»åŠ çœç•¥å·
        return content[:truncate_index] + "\n...ï¼ˆæ¶ˆæ¯è¿‡é•¿ï¼Œå·²æˆªæ–­ï¼‰"
    else:
        # æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„æˆªæ–­ç‚¹ï¼Œç›´æ¥æˆªæ–­
        return content[:MAX_MESSAGE_LENGTH - 10] + "...ï¼ˆæ¶ˆæ¯è¿‡é•¿ï¼Œå·²æˆªæ–­ï¼‰"

def get_last_message_time():
    """è·å–ä¸Šæ¬¡å‘é€æ¶ˆæ¯çš„æ—¶é—´"""
    try:
        if os.path.exists(LAST_MESSAGE_TIME_FILE):
            with open(LAST_MESSAGE_TIME_FILE, 'r') as f:
                timestamp = float(f.read().strip())
                return datetime.datetime.fromtimestamp(timestamp)
        return None
    except Exception as e:
        logging.error(f"è¯»å–ä¸Šæ¬¡å‘é€æ¶ˆæ¯æ—¶é—´æ—¶å‡ºé”™: {str(e)}")
        return None

def save_last_message_time():
    """ä¿å­˜å½“å‰æ—¶é—´ä¸ºä¸Šæ¬¡å‘é€æ¶ˆæ¯çš„æ—¶é—´"""
    try:
        with open(LAST_MESSAGE_TIME_FILE, 'w') as f:
            f.write(str(time.time()))
    except Exception as e:
        logging.error(f"ä¿å­˜ä¸Šæ¬¡å‘é€æ¶ˆæ¯æ—¶é—´æ—¶å‡ºé”™: {str(e)}")

def check_and_mark_published_books(current_books):
    """æ£€æŸ¥å¹¶æ ‡è®°å·²å‡ºä¹¦çš„ä¹¦ç±"""
    try:
        current_titles = [normalize_title(book["title"]) for book in current_books]
        unpublished_books = get_unpublished_books()
        
        published_books = []
        for book in unpublished_books:
            if normalize_title(book["title"]) not in current_titles:
                if mark_book_as_published(book["title"]):
                    published_books.append(book)
        
        return published_books
    except Exception as e:
        logging.error(f"æ£€æŸ¥å¹¶æ ‡è®°å·²å‡ºä¹¦ä¹¦ç±æ—¶å‡ºé”™: {str(e)}")
        return []

def get_unpublished_books():
    """è·å–æ‰€æœ‰æœªå‡ºä¹¦çš„ä¹¦ç±"""
    try:
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        cursor.execute("SELECT title, publish_month, first_seen, last_seen FROM books WHERE is_published = 0")
        result = cursor.fetchall()
        conn.close()
        return [
            {
                "title": row[0],
                "publish_month": row[1],
                "first_seen": row[2],
                "last_seen": row[3]
            }
            for row in result
        ]
    except Exception as e:
        logging.error(f"è·å–æœªå‡ºä¹¦ä¹¦ç±æ—¶å‡ºé”™: {str(e)}")
        return []

def mark_book_as_published(title, publish_date=None):
    """æ ‡è®°ä¹¦ç±ä¸ºå·²å‡ºä¹¦"""
    try:
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        if publish_date:
            cursor.execute(
                "UPDATE books SET is_published = 1, publish_month = ? WHERE title = ?",
                (publish_date, title)
            )
        else:
            cursor.execute(
                "UPDATE books SET is_published = 1 WHERE title = ?",
                (title,)
            )
        conn.commit()
        conn.close()
        logging.info(f"ä¹¦ç±å·²æ ‡è®°ä¸ºå·²å‡ºä¹¦: {title}")
        return True
    except Exception as e:
        logging.error(f"æ ‡è®°ä¹¦ç±ä¸ºå·²å‡ºä¹¦æ—¶å‡ºé”™: {str(e)}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    try:
        setup_logging()
        logging.info("ğŸ“– çˆ¬è™«ç¨‹åºå¯åŠ¨")
        
        # å…ˆè¿è¡Œæ•°æ®åº“æµ‹è¯•
        if not test_database_creation():
            logging.error("âŒ æ•°æ®åº“æµ‹è¯•å¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢")
            return
        
        # çˆ¬å–æ•°æ®
        new_books = []
        current_books = []
        publish_months = set()
        
        for page in range(1, 4):
            result = get_book_titles(page)
            titles = result["titles"]
            publish_month = result["publish_month"]
            publish_months.add(publish_month)
            
            # è®°å½•æ‰€æœ‰ä¹¦ç±
            page_books = []
            today = datetime.datetime.now().strftime("%Y-%m-%d")
            
            for title in titles:
                book_info = {
                    "title": title,
                    "publish_month": publish_month,
                    "first_seen": today,
                    "last_seen": today
                }
                page_books.append(book_info)
            
            current_books.extend(page_books)
            
            # æ£€æŸ¥å¹¶è®°å½•æ–°ä¹¦
            for book in page_books:
                normalized_title = normalize_title(book["title"])
                if not check_book_exists(normalized_title):
                    new_books.append(book)
                    logging.info(f"ğŸ” å‘ç°æ–°ä¹¦: {normalized_title} ({publish_month})")
        
        # è°ƒè¯•ï¼šè¾“å‡ºçˆ¬å–ç»“æœ
        logging.info(f"ğŸ“Š çˆ¬å–å®Œæˆ: {len(current_books)} æœ¬å½“å‰ä¹¦ç±, {len(new_books)} æœ¬æ–°ä¹¦")
        
        # æ·»åŠ æ–°ä¹¦åˆ°æ•°æ®åº“
        if new_books:
            added_count = batch_add_books(new_books)
            logging.info(f"âœ… æ•°æ®åº“æ›´æ–°: æ·»åŠ äº† {added_count} æœ¬æ–°ä¹¦")
        else:
            logging.info("ğŸ“­ æ²¡æœ‰å‘ç°æ–°ä¹¦")
        
        # æ£€æŸ¥å¹¶æ ‡è®°å·²å‡ºä¹¦çš„ä¹¦ç±
        published_books = check_and_mark_published_books(current_books)
        if published_books:
            logging.info(f"ğŸ“¦ å‘ç° {len(published_books)} æœ¬å·²å‡ºä¹¦")
            
            # æ„å»ºå·²å‡ºä¹¦é€šçŸ¥æ¶ˆæ¯
            publish_message = "### ğŸ“¦ æ–°ä¹¦åˆ°è´§é€šçŸ¥\n\n"
            for book in published_books:
                publish_message += f"- **{book['title']}** (é¢„è®¡å‡ºç‰ˆæœˆä»½: {book['publish_month']})\n"
            
            # å‘é€é€šçŸ¥
            if WECHAT_WORK_WEBHOOK:
                send_combined_message("æ–°ä¹¦åˆ°è´§é€šçŸ¥", publish_message)
        else:
            logging.info("ğŸ“­ æ²¡æœ‰ä¹¦ç±æ ‡è®°ä¸ºå·²å‡ºä¹¦")
        
        # è·å–æ‰€æœ‰æœªå‡ºç‰ˆçš„ä¹¦ç±
        all_unpublished_books = get_unpublished_books()
        logging.info(f"ğŸ“š ç›®å‰å…±æœ‰ {len(all_unpublished_books)} æœ¬æœªå‡ºç‰ˆçš„ä¹¦ç±")
        
        # æŒ‰å‡ºç‰ˆæœˆä»½åˆ†ç»„
        books_by_month = {}
        for book in all_unpublished_books:
            month = book["publish_month"]
            if month not in books_by_month:
                books_by_month[month] = []
            books_by_month[month].append(book)
        
        # æ„å»ºç­‰å¾…åˆ—è¡¨æ¶ˆæ¯
        if books_by_month and WECHAT_WORK_WEBHOOK:
            waiting_message = "### ğŸ“š å¾…å‡ºç‰ˆä¹¦ç±åˆ—è¡¨\n\n"
            
            # æŒ‰æœˆä»½æ’åº
            sorted_months = sorted(books_by_month.keys(), key=lambda x: (x.split('~')[0], x))
            
            for month in sorted_months:
                books = books_by_month[month]
                waiting_message += f"#### {month} ({len(books)}æœ¬)\n"
                
                # æŒ‰é¦–æ¬¡å‘ç°æ—¶é—´æ’åº
                books_sorted = sorted(books, key=lambda x: x["first_seen"])
                
                for i, book in enumerate(books_sorted, 1):
                    days_waiting = (datetime.datetime.now() - datetime.datetime.strptime(book["first_seen"], "%Y-%m-%d")).days
                    waiting_message += f"{i}. **{book['title']}** (ç­‰å¾…{days_waiting}å¤©)\n"
                
                waiting_message += "\n"
            
            # å‘é€ç­‰å¾…åˆ—è¡¨é€šçŸ¥
            send_combined_message("å¾…å‡ºç‰ˆä¹¦ç±åˆ—è¡¨", waiting_message)
        
        logging.info("âœ… çˆ¬è™«ç¨‹åºæ‰§è¡Œå®Œæˆ")
        
    except Exception as e:
        import traceback
        logging.error(f"âŒ ç¨‹åºå´©æºƒ: {str(e)}")
        logging.error(f"å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}")
        raise

if __name__ == "__main__":
    main()
